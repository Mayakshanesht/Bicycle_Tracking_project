{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic_segmentation_drivableAreaNetwork.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPuUDJuyZN9IGhK/IFYpmvu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mayakshanesht/Bicycle_Tracking_project/blob/main/Semantic_segmentation_drivableAreaNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yohg4QFCl9i",
        "outputId": "924aca82-1fb6-4730-a4b9-367fa5e2f1f9"
      },
      "source": [
        "#lets mount drive on the google colab\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/segmentation/segmentation_course-master/dataset/dataset\")\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "images_3000_160.p  labels_3000_160.p\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Q5U77wDmXZ"
      },
      "source": [
        "#lets import important python libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pickle\n",
        "import cv2\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57UJm-PDElqO",
        "outputId": "46a3d9af-7ccc-4bcd-e778-a8407542befe"
      },
      "source": [
        "#lets install tensorflow\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHN8IPhoFBlZ"
      },
      "source": [
        "with open('images_3000_160.p','rb') as f:\n",
        "  images1=pickle.load(f)\n",
        "  images=images1[0:100]\n",
        "with open('labels_3000_160.p','rb') as f:\n",
        "  labels1=pickle.load(f) \n",
        "  labels=labels1[0:100]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1O9ujCQuVqv"
      },
      "source": [
        "label1=[]\n",
        "for label in labels:\n",
        "  for x in range(label.shape[0]):\n",
        "    for y in range(label.shape[1]):\n",
        "      if(np.all(label[x][y]==[0,0,0])):\n",
        "        label[x][y]=[0,1,0]\n",
        "        label1.append(label)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duUl39Ccv-TF"
      },
      "source": [
        "plt.imshow(label1[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZs8CwyVvzGk",
        "outputId": "879821be-596d-4f45-de04-37d654b79631"
      },
      "source": [
        "label1[0].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 160, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x80Kt4IuyKJ0"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59X9dnISyd5d"
      },
      "source": [
        "images=np.array(images)\n",
        "label1=np.array(label1)\n",
        "#images,labels1=shuffle(images,label1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFX5GujlyzS5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "05673e68-2d8f-411f-9180-b9450523f86a"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(images, label1, test_size=0.15)\n",
        "n_train = len(X_train)\n",
        "n_val = len(X_val)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-89471e1620ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2116\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid parameters passed: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [3000, 31982214]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2O8GpysTOGk"
      },
      "source": [
        "from tensorflow.python import keras\n",
        "from tensorflow.python.keras.models import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Activation, Dropout, Conv2D, MaxPooling2D, Reshape, Input, add\n",
        "from tensorflow.python.keras.layers import UpSampling2D, Conv2DTranspose\n",
        "from tensorflow.python.keras.layers.normalization import BatchNormalization\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras import regularizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_iDhxrbTjLy"
      },
      "source": [
        "def other_model_dropout(pool_size=(2,2), input_shape=(80,160,3)):\n",
        "    input_x = Input(shape=(80,160,3))\n",
        "    x = BatchNormalization(input_shape=input_shape)(input_x)\n",
        "    x = Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x1 = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=pool_size)(x1)\n",
        "    x = Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x2 = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = MaxPooling2D(pool_size=pool_size)(x2)\n",
        "    x = Conv2D(128, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(128, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x3 = Dropout(0.2)(x)\n",
        "    x = add([x3, x])\n",
        "    x = MaxPooling2D(pool_size=pool_size)(x3)\n",
        "    x = UpSampling2D(size=pool_size, interpolation='bilinear')(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = UpSampling2D(size=pool_size, interpolation='bilinear')(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = add([x2, x])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = UpSampling2D(size=pool_size, interpolation='bilinear')(x)\n",
        "    x = Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2DTranspose(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2DTranspose(8, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(3, (1,1), padding='valid', strides=(1,1), activation='softmax')(x)\n",
        "    return Model(input_x,x)\n",
        "model = other_model_dropout()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISJmQ1PdT3e4"
      },
      "source": [
        "def encoder_decoder_skip_connections(input_shape= (160,80,3), pool_size=(2,2), dropout_rate=0.5):\n",
        "    \"\"\"\n",
        "    Here, build your killer segmentation network.\n",
        "    Use any technique that can be useful.\n",
        "    Credits for the architecture: https://towardsdatascience.com/lane-detection-with-deep-learning-part-2-3ba559b5c5af\n",
        "    \"\"\"\n",
        "    # ENCODER\n",
        "    input_x = Input(shape=(80,160,3))\n",
        "    x1 = BatchNormalization(input_shape=input_shape)(input_x)\n",
        "    ## CONV 1\n",
        "    x = Conv2D(8, (3, 3), strides = (1,1), activation='relu', padding='valid')(x1)\n",
        "    ## CONV 2 + SKIP CONNECTION\n",
        "    x = Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "    ## CONV 3\n",
        "    x = Conv2D(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    ## CONV4 + SKIP CONNECTION\n",
        "    x2 = Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x2)\n",
        "    ## CONV5\n",
        "    x = Conv2D(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "    ## CONV6\n",
        "    x = Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    ## CONV7\n",
        "    x = Conv2D(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    x = MaxPooling2D(pool_size=pool_size)(x)\n",
        "    # DECODER\n",
        "    ## UPSAMPLING 7\n",
        "    x = UpSampling2D(size=pool_size, interpolation='nearest')(x)\n",
        "    x = Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    ## UPSAMPLING 6\n",
        "    x = Conv2DTranspose(64, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    ## UPSAMPLING 5\n",
        "    x = UpSampling2D(size=pool_size, interpolation='nearest')(x)\n",
        "    x = Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    ## UPSAMPLING 4\n",
        "    x = add([x2, x])\n",
        "    x = Conv2DTranspose(32, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    ## UPSAMPLING 3\n",
        "    x = Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    ## UPSAMPLING 2\n",
        "    x = UpSampling2D(size=pool_size, interpolation='nearest')(x)\n",
        "    x = Conv2DTranspose(16, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "\n",
        "    ## UPSAMPLING 1\n",
        "    x = Conv2DTranspose(3, (3, 3), padding='valid', strides=(1,1), activation = 'relu')(x)\n",
        "    x = add([x1, x])\n",
        "    x = Conv2D(3, (1,1), padding=('valid'), strides=(1,1), activation='softmax')(x)\n",
        "    return Model(input_x,x)\n",
        "model = encoder_decoder_skip_connections()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WZhWUOVT5kF"
      },
      "source": [
        "batch_size = 32\n",
        "epochs = 10\n",
        "pool_size = (2, 2)\n",
        "learning_rate= 0.001\n",
        "steps_per_epoch=len(X_train)/batch_size\n",
        "input_shape = X_train.shape[1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ANm8ViiUDeO"
      },
      "source": [
        "# Using a generator to help the model use less data\n",
        "# Channel shifts help with shadows slightly\n",
        "datagen = ImageDataGenerator(channel_shift_range=0.2)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Compiling and training the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(\n",
        "    learning_rate=learning_rate, amsgrad=False, name=\"Adam\"), loss='categorical_crossentropy')\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), steps_per_epoch=steps_per_epoch, \n",
        "          epochs=epochs, verbose=1, validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "# Save model architecture and weights\n",
        "#model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaeMtKk9UNoZ"
      },
      "source": [
        "print(history.history.keys())\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFfcoHD8VDdZ"
      },
      "source": [
        "def rgb_channel(img, thresholding=False, thresh=230):\n",
        "    \"\"\"Threshold the re-drawn images\"\"\"\n",
        "    image = np.copy(img)\n",
        "    if thresholding:\n",
        "        ret, image = cv2.threshold(image, thresh, 255, cv2.THRESH_BINARY)\n",
        "    R = image[:,:,0]\n",
        "    G = image[:,:,1]\n",
        "    B = image[:,:,2]\n",
        "    return R,G,B"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSssoMgKU5V0"
      },
      "source": [
        "def run(input_image):\n",
        "    h,w,d = input_image.shape\n",
        "    network_image = input_image.copy()\n",
        "    network_image = cv2.resize(network_image, (160,80), interpolation=cv2.INTER_AREA)\n",
        "    network_image = network_image[None,:,:,:]\n",
        "    prediction = model.predict(network_image)[0]*255\n",
        "    R,G,B = rgb_channel(prediction)\n",
        "    blank = np.zeros_like(R).astype(np.uint8)\n",
        "    lane_image = np.dstack((R,blank, B))\n",
        "    lane_image = cv2.resize(lane_image, (w,h))\n",
        "    result = cv2.addWeighted(input_image, 1, lane_image.astype(np.uint8), 1, 0)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRXAlDr5VCVF"
      },
      "source": [
        "import glob\n",
        "img_array = []\n",
        "for filename in sorted(glob.glob('data_2/*.png')):\n",
        "    img = cv2.imread(filename)\n",
        "    height, width, layers = img.shape\n",
        "    size = (width,height)\n",
        "    img_array.append(img)\n",
        " \n",
        "out = cv2.VideoWriter('videos/project_2.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, size)\n",
        " \n",
        "for i in range(len(img_array)):\n",
        "    out.write(img_array[i])\n",
        "out.release()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P8LPaD6VVXk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}